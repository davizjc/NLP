{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1isOxN-rfU6",
        "outputId": "99773c29-2c0e-4def-a4a6-8caca9c97a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV , train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping , ModelCheckpoint\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n"
      ],
      "metadata": {
        "id": "ADhu2sJ4_VlD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = []\n",
        "text = []\n",
        "subcat = []\n",
        "\n",
        "with open('/content/drive/MyDrive/intern/nlp/allresult2.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(' ', 2)  # Splitting on the first two spaces\n",
        "        if len(parts) == 3:  # Ensure there are three parts: keyword, first part of text, second part of text\n",
        "            keyword.append(parts[0])\n",
        "            text.append(parts[1])\n",
        "            subcat.append(parts[2])\n",
        "\n",
        "data = pd.DataFrame({'keyword': keyword, 'text': text, 'subcat': subcat})\n"
      ],
      "metadata": {
        "id": "mpubwCCc_ans"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn categorical labels into a numeric\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(data['keyword'])\n",
        "print(y)"
      ],
      "metadata": {
        "id": "vQ89gLgg3gyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ed056c-b1ce-4c14-d70b-def009b0736e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_X, Test_X, Train_Y, Test_Y = train_test_split(data['text'], y, test_size=0.15, random_state=42)\n",
        "# print(Train_X)"
      ],
      "metadata": {
        "id": "s3XPL1tM3oH6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converts text into numerical values, specifically into a matrix of TF-IDF features.\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(data['text'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "print(Train_X_Tfidf)   #(row,col    TF-IDF score)\n",
        "# When a document has only one word, the TF-IDF score for that word in that specific document will be 1.0.\n",
        "\n",
        "# print(Tfidf_vect.vocabulary_)\n"
      ],
      "metadata": {
        "id": "e3udT1NbVzC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20eae80f-631c-4f9b-bc81-8763496f8509"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 356)\t1.0\n",
            "  (1, 337)\t0.3263678581046755\n",
            "  (1, 328)\t0.3263678581046755\n",
            "  (1, 288)\t0.36890434087673707\n",
            "  (1, 260)\t0.36890434087673707\n",
            "  (1, 228)\t0.36890434087673707\n",
            "  (1, 211)\t0.3263678581046755\n",
            "  (1, 157)\t0.36890434087673707\n",
            "  (1, 118)\t0.36890434087673707\n",
            "  (2, 337)\t0.3263678581046755\n",
            "  (2, 328)\t0.3263678581046755\n",
            "  (2, 288)\t0.36890434087673707\n",
            "  (2, 260)\t0.36890434087673707\n",
            "  (2, 228)\t0.36890434087673707\n",
            "  (2, 211)\t0.3263678581046755\n",
            "  (2, 157)\t0.36890434087673707\n",
            "  (2, 118)\t0.36890434087673707\n",
            "  (3, 369)\t1.0\n",
            "  (4, 344)\t0.2316560662621703\n",
            "  (4, 319)\t0.252483301908755\n",
            "  (4, 305)\t0.5236969653251488\n",
            "  (4, 296)\t0.2618484826625744\n",
            "  (4, 285)\t0.2618484826625744\n",
            "  (4, 274)\t0.2618484826625744\n",
            "  (4, 264)\t0.2618484826625744\n",
            "  :\t:\n",
            "  (245, 98)\t1.0\n",
            "  (246, 373)\t1.0\n",
            "  (247, 186)\t0.7400032589036016\n",
            "  (247, 121)\t0.6726032833788793\n",
            "  (248, 91)\t1.0\n",
            "  (249, 97)\t1.0\n",
            "  (250, 374)\t0.7071067811865475\n",
            "  (250, 198)\t0.7071067811865475\n",
            "  (251, 219)\t0.7071067811865475\n",
            "  (251, 204)\t0.7071067811865475\n",
            "  (252, 344)\t0.2316560662621703\n",
            "  (252, 319)\t0.252483301908755\n",
            "  (252, 305)\t0.5236969653251488\n",
            "  (252, 296)\t0.2618484826625744\n",
            "  (252, 285)\t0.2618484826625744\n",
            "  (252, 274)\t0.2618484826625744\n",
            "  (252, 264)\t0.2618484826625744\n",
            "  (252, 253)\t0.2618484826625744\n",
            "  (252, 197)\t0.2618484826625744\n",
            "  (252, 181)\t0.2618484826625744\n",
            "  (252, 172)\t0.2618484826625744\n",
            "  (252, 163)\t0.24456515440446447\n",
            "  (253, 376)\t0.7071067811865475\n",
            "  (253, 82)\t0.7071067811865475\n",
            "  (254, 168)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the training dataset on the NB classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEUL043u2i1G",
        "outputId": "541a6fd0-afd0-41a5-a191-9fd9a0a524d8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy Score ->  42.22222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preprocess the input text\n",
        "input_text = \" ผู้อนุมัติเอกสารสามารถตรวจสอบความถูกต้องของเนื้อหาและมีสิทธิ์ในการแก้ไขเนื้อหาแล้วส่งต่อผู้พิจารณาเอกสารล าดับถัดไป หรือส่งคืนเอกสารให้ผู้ร่างเอกสารด าเนินการแก้ไข\"\n",
        "\n",
        "# Transform the input text using the same Tfidf_vect object that you used for transforming the training data\n",
        "input_text_tfidf = Tfidf_vect.transform([input_text])\n",
        "\n",
        "# 2. Predict using the Naive Bayes model\n",
        "prediction = Naive.predict(input_text_tfidf)\n",
        "\n",
        "# The prediction will directly be the label index, so no need to use np.argmax\n",
        "predicted_label = le.inverse_transform([prediction[0]])  # inverse transform to get the original class label\n",
        "\n",
        "print(f\"Predicted class: {predicted_label[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUT1Ue4SgJGM",
        "outputId": "fcae5ee4-454e-4537-9f7b-9f234f7ff617"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_X_Tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJgV-jDmdpQW",
        "outputId": "b46fab77-e074-4426-cc1c-caf807b2350c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(255, 389)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Test_Y, predictions_NB))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ToFD_PUdufn",
        "outputId": "c5fab4db-6dbb-489d-85e6-0e505ff3e504"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.75      0.53        12\n",
            "           1       0.18      0.20      0.19        10\n",
            "           2       1.00      0.50      0.67        12\n",
            "           3       0.33      0.18      0.24        11\n",
            "\n",
            "    accuracy                           0.42        45\n",
            "   macro avg       0.48      0.41      0.41        45\n",
            "weighted avg       0.50      0.42      0.42        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with hyperparameter tuning\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 1, 10]}\n",
        "svc = svm.SVC()\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(Train_X_Tfidf, Train_Y)\n",
        "\n",
        "# Predict using the best model\n",
        "predictions_SVM = clf.best_estimator_.predict(Test_X_Tfidf)"
      ],
      "metadata": {
        "id": "JmB5HfZAd5Ol"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preprocess the input text\n",
        "input_text = \" ผู้อนุมัติเอกสารสามารถตรวจสอบความถูกต้องของเนื้อหาและมีสิทธิ์ในการแก้ไขเนื้อหาแล้วส่งต่อผู้พิจารณาเอกสารล าดับถัดไป หรือส่งคืนเอกสารให้ผู้ร่างเอกสารด าเนินการแก้ไข\"\n",
        "\n",
        "# Transform the input text using the same Tfidf_vect object that you used for transforming the training data\n",
        "input_text_tfidf = Tfidf_vect.transform([input_text])\n",
        "\n",
        "# 2. Predict using the Naive Bayes model\n",
        "prediction = clf.predict(input_text_tfidf)\n",
        "\n",
        "# The prediction will directly be the label index, so no need to use np.argmax\n",
        "predicted_label = le.inverse_transform([prediction[0]])  # inverse transform to get the original class label\n",
        "\n",
        "print(f\"Predicted class: {predicted_label[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRn07hFAge4t",
        "outputId": "fb2f3def-6c56-46b7-9791-9b8c3615aed7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Test_Y,predictions_SVM))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8NDSVCvd9rs",
        "outputId": "a7abaa1f-c90d-404d-98b8-2e8ff76d0eef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      1.00      0.63        12\n",
            "           1       0.30      0.30      0.30        10\n",
            "           2       1.00      0.42      0.59        12\n",
            "           3       0.25      0.09      0.13        11\n",
            "\n",
            "    accuracy                           0.47        45\n",
            "   macro avg       0.50      0.45      0.41        45\n",
            "weighted avg       0.52      0.47      0.42        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding for the labels\n",
        "y_train_encoded = tf.keras.utils.to_categorical(Train_Y)\n",
        "y_test_encoded = tf.keras.utils.to_categorical(Test_Y)\n"
      ],
      "metadata": {
        "id": "qFoCFlxQ9PH0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(Train_X)\n",
        "# turn word to number same word same number\n",
        "train_sequences = tokenizer.texts_to_sequences(Train_X)\n",
        "print(train_sequences)  #array len(128)\n",
        "test_sequences = tokenizer.texts_to_sequences(Test_X)\n",
        "# print(test_sequences)  arrray len(32)\n",
        "\n",
        "\n",
        "# same length before training the  data\n",
        "Train_X_seq = pad_sequences(train_sequences, maxlen=max_len)\n",
        "Test_X_seq = pad_sequences(test_sequences, maxlen=max_len)\n",
        "\n"
      ],
      "metadata": {
        "id": "GQbWJJzy-NSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b257d94-8bae-4652-bbc1-7ee90326a3e0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[42], [11], [11], [12], [13], [43], [44], [45], [46], [7], [18], [19], [8], [3], [9], [8], [47], [48], [6], [49], [50], [51], [14], [52], [4], [20], [21], [15], [5], [53], [22], [54], [55], [56], [57], [58], [2], [59], [23], [60], [61], [62], [14], [63], [64], [2], [24], [65], [66], [25], [67], [2], [68], [26], [27], [20], [69], [70], [27], [9], [15], [28], [1], [71], [72], [73], [74], [7], [75], [16], [29], [76], [30], [77], [78], [79], [80], [81], [3], [31], [7], [82], [14], [83], [84], [18], [30], [5], [85], [32], [86], [87], [4], [5], [88], [33], [89], [3], [34], [6], [34], [35], [90], [91], [92], [93], [8], [94], [95], [10], [1], [96], [97], [98], [1], [5], [99], [100], [16], [101], [17], [102], [1], [103], [104], [10], [105], [17], [106], [12], [107], [36], [108], [23], [16], [6], [109], [37], [110], [111], [38], [28], [112], [37], [17], [113], [6], [12], [3], [114], [115], [33], [19], [116], [117], [118], [39], [119], [2], [120], [121], [1], [32], [122], [1], [38], [40], [123], [124], [25], [125], [126], [127], [128], [129], [10], [1], [130], [5], [131], [10], [1], [132], [133], [3], [134], [26], [39], [35], [135], [136], [137], [11], [138], [8], [139], [4], [140], [1], [141], [142], [6], [143], [36], [144], [15], [145], [146], [147], [148], [1], [149], [150], [9], [151], [13], [152], [153], [154], [155], [4], [156], [157], [158], [159], [160], [161], [162], [2], [41], [3], [2], [163], [164], [1], [24], [165], [166], [31], [4], [21], [167], [9], [4], [22], [41], [2], [40], [29], [1], [5], [168], [13], [169], [7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the CNN model with added complexity\n",
        "model = tf.keras.models.Sequential([\n",
        "    Embedding(max_words, 128, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train_encoded.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "newModel = tf.keras.models.load_model('/content/drive/MyDrive/code/best_model_nlp_cnn.h5')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Set up the ModelCheckpoint callback\n",
        "checkpoint_path = '/content/drive/MyDrive/code/best_model_nlp4_cnn.h5'\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True)\n",
        "\n",
        "# Train the model with ModelCheckpoint and EarlyStopping callbacks\n",
        "history = model.fit(Train_X_seq, y_train_encoded, validation_data=(Test_X_seq, y_test_encoded), epochs=100, batch_size= 20, callbacks=[checkpoint, early_stopping], shuffle=True)\n",
        "\n",
        "# Extracting accuracy and printing\n",
        "train_accuracies = history.history['accuracy']\n",
        "valid_accuracies = history.history['val_accuracy']\n",
        "max_train_accuracy = max(train_accuracies)\n",
        "max_valid_accuracy = max(valid_accuracies)\n",
        "\n",
        "print(f\"Highest Training Accuracy: {max_train_accuracy*100:.2f}%\")\n",
        "print(f\"Highest Validation Accuracy: {max_valid_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "4WHf1d_THJoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9545053-0534-4b4d-e9a5-570cd53e439f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3986 - accuracy: 0.2353\n",
            "Epoch 1: val_accuracy improved from -inf to 0.22222, saving model to /content/drive/MyDrive/code/best_model_nlp4_cnn.h5\n",
            "13/13 [==============================] - 3s 110ms/step - loss: 1.3986 - accuracy: 0.2353 - val_loss: 1.3880 - val_accuracy: 0.2222\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3927 - accuracy: 0.2667\n",
            "Epoch 2: val_accuracy improved from 0.22222 to 0.35556, saving model to /content/drive/MyDrive/code/best_model_nlp4_cnn.h5\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 1.3927 - accuracy: 0.2667 - val_loss: 1.3870 - val_accuracy: 0.3556\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3910 - accuracy: 0.2275\n",
            "Epoch 3: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 1.3910 - accuracy: 0.2275 - val_loss: 1.3869 - val_accuracy: 0.2222\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3864 - accuracy: 0.2275\n",
            "Epoch 4: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 1.3864 - accuracy: 0.2275 - val_loss: 1.3868 - val_accuracy: 0.2222\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3908 - accuracy: 0.2235\n",
            "Epoch 5: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 1.3908 - accuracy: 0.2235 - val_loss: 1.3867 - val_accuracy: 0.2222\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3868 - accuracy: 0.2392\n",
            "Epoch 6: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 1.3868 - accuracy: 0.2392 - val_loss: 1.3869 - val_accuracy: 0.2222\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3881 - accuracy: 0.2392\n",
            "Epoch 7: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 1.3881 - accuracy: 0.2392 - val_loss: 1.3868 - val_accuracy: 0.2222\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3871 - accuracy: 0.2078\n",
            "Epoch 8: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 1.3871 - accuracy: 0.2078 - val_loss: 1.3872 - val_accuracy: 0.2222\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3863 - accuracy: 0.2275\n",
            "Epoch 9: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 1.3863 - accuracy: 0.2275 - val_loss: 1.3870 - val_accuracy: 0.2444\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3869 - accuracy: 0.2471\n",
            "Epoch 10: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 2s 138ms/step - loss: 1.3869 - accuracy: 0.2471 - val_loss: 1.3872 - val_accuracy: 0.2444\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3880 - accuracy: 0.2314\n",
            "Epoch 11: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 2s 131ms/step - loss: 1.3880 - accuracy: 0.2314 - val_loss: 1.3880 - val_accuracy: 0.2222\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3886 - accuracy: 0.2627\n",
            "Epoch 12: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 1.3886 - accuracy: 0.2627 - val_loss: 1.3876 - val_accuracy: 0.2222\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3880 - accuracy: 0.2275\n",
            "Epoch 13: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 1.3880 - accuracy: 0.2275 - val_loss: 1.3872 - val_accuracy: 0.2222\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3858 - accuracy: 0.2510\n",
            "Epoch 14: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 1.3858 - accuracy: 0.2510 - val_loss: 1.3871 - val_accuracy: 0.2222\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3868 - accuracy: 0.2510\n",
            "Epoch 15: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 76ms/step - loss: 1.3868 - accuracy: 0.2510 - val_loss: 1.3870 - val_accuracy: 0.2222\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3876 - accuracy: 0.2627\n",
            "Epoch 16: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 1.3876 - accuracy: 0.2627 - val_loss: 1.3870 - val_accuracy: 0.2222\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3878 - accuracy: 0.2824\n",
            "Epoch 17: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 1.3878 - accuracy: 0.2824 - val_loss: 1.3867 - val_accuracy: 0.2222\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3869 - accuracy: 0.2392\n",
            "Epoch 18: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 81ms/step - loss: 1.3869 - accuracy: 0.2392 - val_loss: 1.3866 - val_accuracy: 0.2222\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3846 - accuracy: 0.2431\n",
            "Epoch 19: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 1.3846 - accuracy: 0.2431 - val_loss: 1.3857 - val_accuracy: 0.2222\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3828 - accuracy: 0.3137\n",
            "Epoch 20: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 1.3828 - accuracy: 0.3137 - val_loss: 1.3798 - val_accuracy: 0.3333\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.3607 - accuracy: 0.3608\n",
            "Epoch 21: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 1.3607 - accuracy: 0.3608 - val_loss: 1.3371 - val_accuracy: 0.2889\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.1999 - accuracy: 0.4667\n",
            "Epoch 22: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 2s 129ms/step - loss: 1.1999 - accuracy: 0.4667 - val_loss: 1.2451 - val_accuracy: 0.3111\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0408 - accuracy: 0.4745\n",
            "Epoch 23: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 2s 131ms/step - loss: 1.0408 - accuracy: 0.4745 - val_loss: 1.1841 - val_accuracy: 0.3556\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.9109 - accuracy: 0.4941\n",
            "Epoch 24: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 2s 131ms/step - loss: 0.9109 - accuracy: 0.4941 - val_loss: 1.1820 - val_accuracy: 0.3333\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.8497 - accuracy: 0.4980\n",
            "Epoch 25: val_accuracy did not improve from 0.35556\n",
            "13/13 [==============================] - 1s 104ms/step - loss: 0.8497 - accuracy: 0.4980 - val_loss: 1.1630 - val_accuracy: 0.3556\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.7806 - accuracy: 0.5137\n",
            "Epoch 26: val_accuracy improved from 0.35556 to 0.55556, saving model to /content/drive/MyDrive/code/best_model_nlp4_cnn.h5\n",
            "13/13 [==============================] - 2s 124ms/step - loss: 0.7806 - accuracy: 0.5137 - val_loss: 1.1084 - val_accuracy: 0.5556\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.7625 - accuracy: 0.6902\n",
            "Epoch 27: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 81ms/step - loss: 0.7625 - accuracy: 0.6902 - val_loss: 1.1540 - val_accuracy: 0.4889\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.7322 - accuracy: 0.6941\n",
            "Epoch 28: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 0.7322 - accuracy: 0.6941 - val_loss: 1.0743 - val_accuracy: 0.5333\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.6855 - accuracy: 0.7255\n",
            "Epoch 29: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 0.6855 - accuracy: 0.7255 - val_loss: 1.1316 - val_accuracy: 0.5333\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.6321 - accuracy: 0.6980\n",
            "Epoch 30: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 0.6321 - accuracy: 0.6980 - val_loss: 1.2080 - val_accuracy: 0.4889\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.6824\n",
            "Epoch 31: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.6354 - accuracy: 0.6824 - val_loss: 1.2917 - val_accuracy: 0.5111\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.6137 - accuracy: 0.7176\n",
            "Epoch 32: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 0.6137 - accuracy: 0.7176 - val_loss: 1.3713 - val_accuracy: 0.4667\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.6941\n",
            "Epoch 33: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 0.5773 - accuracy: 0.6941 - val_loss: 1.4516 - val_accuracy: 0.5111\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.5742 - accuracy: 0.6863\n",
            "Epoch 34: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 95ms/step - loss: 0.5742 - accuracy: 0.6863 - val_loss: 1.4500 - val_accuracy: 0.4667\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.5246 - accuracy: 0.7216\n",
            "Epoch 35: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 2s 132ms/step - loss: 0.5246 - accuracy: 0.7216 - val_loss: 1.6658 - val_accuracy: 0.5111\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.5742 - accuracy: 0.7294\n",
            "Epoch 36: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 2s 134ms/step - loss: 0.5742 - accuracy: 0.7294 - val_loss: 1.6155 - val_accuracy: 0.4222\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.7608\n",
            "Epoch 37: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 2s 132ms/step - loss: 0.4902 - accuracy: 0.7608 - val_loss: 1.6733 - val_accuracy: 0.4222\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.7686\n",
            "Epoch 38: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.4669 - accuracy: 0.7686 - val_loss: 1.7659 - val_accuracy: 0.4667\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.7490\n",
            "Epoch 39: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 0.5049 - accuracy: 0.7490 - val_loss: 1.8835 - val_accuracy: 0.4444\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.7647\n",
            "Epoch 40: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 84ms/step - loss: 0.4663 - accuracy: 0.7647 - val_loss: 1.9600 - val_accuracy: 0.4444\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.7725\n",
            "Epoch 41: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 0.4761 - accuracy: 0.7725 - val_loss: 1.9677 - val_accuracy: 0.4444\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4590 - accuracy: 0.7725\n",
            "Epoch 42: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 82ms/step - loss: 0.4590 - accuracy: 0.7725 - val_loss: 2.1321 - val_accuracy: 0.4444\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.7765\n",
            "Epoch 43: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 82ms/step - loss: 0.4349 - accuracy: 0.7765 - val_loss: 2.2313 - val_accuracy: 0.4444\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.7490\n",
            "Epoch 44: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 84ms/step - loss: 0.4672 - accuracy: 0.7490 - val_loss: 2.0258 - val_accuracy: 0.4444\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4395 - accuracy: 0.8000\n",
            "Epoch 45: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 103ms/step - loss: 0.4395 - accuracy: 0.8000 - val_loss: 2.1528 - val_accuracy: 0.4444\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4090 - accuracy: 0.7804\n",
            "Epoch 46: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 0.4090 - accuracy: 0.7804 - val_loss: 2.2263 - val_accuracy: 0.4444\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.7765\n",
            "Epoch 47: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 110ms/step - loss: 0.4321 - accuracy: 0.7765 - val_loss: 2.1506 - val_accuracy: 0.4444\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.8078\n",
            "Epoch 48: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 0.4030 - accuracy: 0.8078 - val_loss: 2.1251 - val_accuracy: 0.4444\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.7961\n",
            "Epoch 49: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 2s 134ms/step - loss: 0.4015 - accuracy: 0.7961 - val_loss: 2.2027 - val_accuracy: 0.4444\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8039\n",
            "Epoch 50: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 2s 129ms/step - loss: 0.3931 - accuracy: 0.8039 - val_loss: 2.2308 - val_accuracy: 0.4444\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - ETA: 0s - loss: 0.3842 - accuracy: 0.8039\n",
            "Epoch 51: val_accuracy did not improve from 0.55556\n",
            "13/13 [==============================] - 1s 82ms/step - loss: 0.3842 - accuracy: 0.8039 - val_loss: 2.1774 - val_accuracy: 0.4444\n",
            "Highest Training Accuracy: 80.78%\n",
            "Highest Validation Accuracy: 55.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/code/best_model_nlp4_cnn.h5')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = loaded_model.evaluate(Test_X_seq, y_test_encoded, batch_size=3)\n",
        "print(f\"Model Accuracy on Test Data: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJIE0wAzHXpx",
        "outputId": "7fdf125d-fc85-4f35-b318-600b61ffdc29"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1084 - accuracy: 0.5556\n",
            "Model Accuracy on Test Data: 55.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probabilities = model.predict(Test_X_seq) #Get the predictions for the test data\n",
        "y_pred_labels = np.argmax(y_pred_probabilities, axis=1) # Convert probability distributions to class labels\n",
        "\n",
        "\n",
        "print(classification_report(Test_Y, y_pred_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mYDm6blJMiX",
        "outputId": "1daf3313-ccf3-4143-8731-9233ea1a315c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      1.00      0.62        12\n",
            "           1       0.00      0.00      0.00        10\n",
            "           2       1.00      0.42      0.59        12\n",
            "           3       0.62      0.73      0.67        11\n",
            "\n",
            "    accuracy                           0.56        45\n",
            "   macro avg       0.51      0.54      0.47        45\n",
            "weighted avg       0.54      0.56      0.48        45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preprocess the input text\n",
        "input_text = \" ผู้อนุมัติเอกสารสามารถตรวจสอบความถูกต้องของเนื้อหาและมีสิทธิ์ในการแก้ไขเนื้อหาแล้วส่งต่อผู้พิจารณาเอกสารล าดับถัดไป หรือส่งคืนเอกสารให้ผู้ร่างเอกสารด าเนินการแก้ไข\"\n",
        "\n",
        "# Tokenize and pad\n",
        "sequence = tokenizer.texts_to_sequences([input_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "print(sequence)\n",
        "\n",
        "# 2. Predict using the model\n",
        "prediction = model.predict(padded_sequence)\n",
        "# 3. Decode the prediction\n",
        "\n",
        "predicted_label_index = np.argmax(prediction, axis=1)[0]\n",
        "predicted_label = le.inverse_transform([predicted_label_index])\n",
        "\n",
        "print(f\"Predicted class: {predicted_label[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1LwLQgCZh_F",
        "outputId": "76514bde-bb5c-4e2d-a971-095836fa87d7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted class: 1\n"
          ]
        }
      ]
    }
  ]
}